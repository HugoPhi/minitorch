{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85cb1542-6b6c-493b-9c9c-288e17d47095",
   "metadata": {},
   "source": [
    "# How to use AutoGrad by Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f3858d-c4e7-424f-a2ee-d5cfe6c8181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, tree, grad, jit\n",
    "from plugins.minitorch.utils import softmax, cross_entropy_loss, one_hot\n",
    "\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ada4c2-1667-4a85-8e54-6239e286d947",
   "metadata": {},
   "source": [
    "How to use grad decides how we manage our trainable parameters.   \n",
    "Fortunatly, Jax provide us a very convenient way calculate grad——by pass a dict of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d290f4f-e30c-434e-bf7e-681a18fb033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * shape of x_train: (1000, 2)\n",
      " * shape of y_train: (1000, 4)\n",
      " * shape of params dict\n",
      "{\n",
      "    \"fc:0\": {\n",
      "        \"b\": \"(3,)\",\n",
      "        \"w\": \"(2, 3)\"\n",
      "    },\n",
      "    \"fc:1\": {\n",
      "        \"b\": \"(4,)\",\n",
      "        \"w\": \"(3, 4)\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'fc:0': {\n",
    "        'w': jnp.ones((2, 3)),\n",
    "        'b': jnp.ones((3,)),\n",
    "    },\n",
    "    'fc:1': {\n",
    "        'w': jnp.ones((3, 4)),\n",
    "        'b': jnp.ones((4,)),\n",
    "    },\n",
    "}\n",
    "\n",
    "x_train = random.normal(key, (1000, 2))\n",
    "y_train = random.randint(key, (1000, 1), 0, 3)\n",
    "y_train = one_hot(y_train, 4)\n",
    "print(f' * shape of x_train: {x_train.shape}')\n",
    "print(f' * shape of y_train: {y_train.shape}')\n",
    "\n",
    "params_shape = tree.map(lambda x: str(x.shape), params)\n",
    "\n",
    "import json\n",
    "print(' * shape of params dict')\n",
    "print(json.dumps(params_shape, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6b740-91a0-4e87-9e27-e7dbfdd203e6",
   "metadata": {},
   "source": [
    "Here is a small change, that is the position of key 'w' & 'b' exchanged. The reason for this problem is: Jax tools always use multi-thread technique to optimize the calculation of dict, and it my not use lock to keep the order. So make you always use key to access value of key instead of use ways like: \n",
    "\n",
    "```python\n",
    "w, b = params.values()\n",
    "```\n",
    "\n",
    "or this small bug will becomes a fatal threat for your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a5e7c5-6414-46ff-b0ba-d5a2ca9bf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, params):\n",
    "    res = x @ params['fc:0']['w'] + params['fc:0']['b']\n",
    "    res = jnp.maximum(0, res)\n",
    "    res = res @ params['fc:1']['w'] + params['fc:1']['b']\n",
    "    res = jnp.maximum(0, res)\n",
    "\n",
    "    return softmax(res)\n",
    "\n",
    "_loss = lambda params: cross_entropy_loss(y_train, forward(x_train, params))\n",
    "_loss = jit(_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e9b2ae-5022-46df-be3a-1871fa17914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * time cost: 0.003015279769897461 s\n",
      "{\n",
      "    \"fc:0\": {\n",
      "        \"b\": \"(3,)\",\n",
      "        \"w\": \"(2, 3)\"\n",
      "    },\n",
      "    \"fc:1\": {\n",
      "        \"b\": \"(4,)\",\n",
      "        \"w\": \"(3, 4)\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()\n",
    "grad_res = grad(_loss, argnums=0)(params)\n",
    "print(f' * time cost: {time.time() - s} s')\n",
    "\n",
    "grad_res_shape = tree.map(lambda x: str(x.shape), grad_res)\n",
    "print(json.dumps(grad_res_shape, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33082b-f1e8-4b89-80f4-f8e52512de87",
   "metadata": {},
   "source": [
    "Here is a very simple MLP case. As you can see, we get a gradient dict of trainable parameters we inited before. And then you can apply this result to GD algorithms like SGD, Adam... easy right?  \n",
    "But this is also not what we want. This kind of initalization and optimization is very complex. So we can apply Pipeline Pattern to make it more easy to manage this procedure for users:  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../assets/notebook_docs/minitorch.svg\" alt=\"Overview of framework\", width=\"50%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "Overview of Framework\n",
    "</p>\n",
    "\n",
    "In the figure, red line represents for the process of init parameters.    \n",
    "- Step 1, we get input parameters of users, such as 'input dim', 'output channel' ..., then use parameterizer in nn.layers to convert it into dict;\n",
    "- Step 2, then we pass it into Initer to filter out trainable parameter dict;\n",
    "- Step 3, create Jax array to contain the trainable parameters & return real trainable parameter dict just like what we do before.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
